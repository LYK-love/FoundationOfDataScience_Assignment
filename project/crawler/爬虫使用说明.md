# 爬虫使用说明
- 主要接口：

  ```python
  def multi_thread(tasks: list, project_path: str):
  ```

  - 参数列表：

    | 参数         | 解释                                                         |
    | ------------ | ------------------------------------------------------------ |
    | `tasks`      | 任务列表, 其中的每一项代表一个任务, 用一个三元组表示: (请求头[^1], 起始页, 终止页), 最好保证起始页和终止页都是5的整数倍, 起始页和终止页的范围在1100-2560之间 |
    | `proje_path` | 是该项目最顶层的`project`文件夹的绝对路径, 你可以在pyCharm中右击`project`目录, 选择右键菜单中的copy path选项, 然后选择absolute path以获得绝对路径, 值得注意的是, 你需要将这个路径中的所有`\`字符改为`\\`, 才能使得程序正常运行 |

    [^1]:请求的结构, 请求头是什么等等的知识在`Python爬虫——基础知识.md`中可以找到, 具体获取方法是: 先用浏览器登陆微博账号, 然后进入浏览器的开发人员模式, 进入微博主页, 找到请求头, 复制`cookie`和`user-agent`这两项到代码中相应的位置, 代码中请求头的具体格式可以仿照`util.headers_3`

  - 功能简介

    该方法可以根据你传入的任务数量(具体为tasks列表中的元组数量)来开启相应数量的线程, 爬取微博页面, 如果你有多个无用的微博账号, 可以把她们的请求头(主要是cookie)复制下来, 分别放在不同的task中,使用不同的cookie来爬取不同的页面, 这样爬取速度会快一些.

    爬取页面的url来自`resorce`文件夹, 我将他们分成每5页一个文件(文件名为`weibo_urls(页码).txt`), 爬虫仅仅在完成爬取完一个文件中的所有url时才会为它做上标记, 以防止重复爬取, 所以如果爬虫爬取到某个`weibo_urls(页码).txt`的一半时出现异常, 那么最好手动删除文件中那些已经完整爬取的url, 保证不会重复

- 其他参数调节

  - 页面爬取等待时间

    下面是`crawler.src.news_parser.weibo_parser`中的方法代码, 调整位置我已经用注释标出

    ```python
     def pages_of(self, url: str):
            pages = []
            first = util.get_weibo_page("{base}?page={pageNO}".format(base=url, pageNO=1), headers=self.headers)
            pages.append(first)
            temp = first.find(attrs={'name': 'mp'})
            end = 1
            if temp is not None:
                end = int(temp.attrs['value'])
            else:
                print(f"没有页数信息，默认1页，请确认：{url}")
                return pages
    
            for i in range(2, end + 1):
                sec = (0.05 + random.random() * 0.01) ## 这里是爬取页面的等待时间, 你可以根据情况调整
                print("正在爬取{url}的第{pageNO}/{total}页".format(url=url, pageNO=i, total=end))
                time.sleep(sec)
                cur = util.get_weibo_page("{base}?page={pageNO}".format(base=url, pageNO=i), headers=self.headers)
                pages.append(cur)
            return pages
    ```

  - 发生404错误以后的等待时间

    ```python
    def get_weibo_page(url: str, headers=headers_3) -> BeautifulSoup:
        response = requests.get(url, headers=headers)
        # print("状态码：{code}".format(code=response.status_code))
        while response.status_code != 200:
            sec = (50 + random.random() * 100) ## 和之前类似, 你可以根据需要调整这个时间
            print("出现错误:{code}，等待{second}秒...".format(code=response.status_code, second=round(sec, 5)))
            time.sleep(sec)
            response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'lxml')
        return soup
    ```

    

- 其他说明

  由于微博的特性, 过快或过多的爬取可能会遭遇反爬, 具体表现就是服务器拒绝访问, 这种情况的出现并不代表号被封了, 但因为我们的账号数量有限, 还是需要谨慎一些, 爬取过程中如果遇到此类问题, 我的默认处理是爬虫随机停止50-150秒, 然后继续访问之前出错的页面, 在爬取页数达到一定数量的时候, 这种情况出现的次数会很多, 这意味着程序的效率会急剧下降, 这时你可以选择记录当前完整爬取的最后一个url, 然后退出程序, 等一段时间在继续爬...