# 语料库构建需求说明

### 概要
总的来说，语料库的构建模块需要一个主要方法，还有两个封装。其中主要方法的工作就是把一些比较粗糙的句子变成一个干净整洁的语料库；第一个封装是针对与助教提供的`.json`形式数据的封装；另外一个是针对于本组项目使用的News类的封装[^1]。

[^1]:理论上来讲这两个都不算太难，`.json`数据应该可以找到相应的json解析库，而News类就更简单了，直接访问对象的相应字段就可以了（当然，在那之前还需要先调用一下相应的方法来将.md文件解析成News对象，这个方法我已经写好了）

### 详情

- 主方法

  - 输入形式

    为了使方法能有更广泛的适用性, 应该考虑把主构建方法与具体内容的解析分开. 因此, 主方法的输入形式最好尽可能的简单一些, 比如==以换行分割不同的句子==. 这样就可以通过对主方法不同形式的封装来从不同格式的数据中构建语料库了!

    此外, 主方法还应该接收一个停用词表, 并且按照停用词表筛除相应的停用词[^2]

    [^2]:接收停用词表并不意味着你需要自己决定什么词需要被停用, 事实上在开始的时候, 我们会有一个现成的停用词表, 或者直接用一个空的停用词表, 然后进行一次分词和构建词向量, 然后通过一次数据分析筛选出更多的停用词.

    

  - 功能

    主方法的任务是将一堆粗糙的句子变成一个纯净的语料库, 同时进行分词.

    - 什么是"纯净"?

      这是指句子中==没有任何标点==, 如果句子很长, 但是其中有很多类似逗号等具有分割性质的标点, 那么甚至可以按照这些标点来将一个句子分成很多个比较小的句子, 此外, 还需要==按照需求去除停用词==

    - 分词

      在去除标点以后分词应该会变得很简单, 只要调用一下库函数即可

- 面向`.json`格式数据的封装

  弄这个封装并不是我闲得慌, 而是因为爬虫效率不太高, 所以我希望爬取数据的过程和数据分析的过程能够同时进行, 因此我们就要利用主教的数据集.

  - 输入形式

    `.json`格式的文件, 也就是由主教所提供的数据集

  - 功能

    按照主教数据集的格式, 将新闻的内容和评论解析成为主方法所需要的形式然后调用主方法进行语料库构建

    >如果你想的话, 也可以写一个专门将`.json`解析成`News`对象的方法, 方便我们日后在主教的数据集上进行各种实验

- 面向`News`对象的数据格式封装

  - 输入形式

    爬虫程序生成的`.md`文件(路径列表)
    通过调用相应的解析方法将文件内容解析成`News`对象

  - 功能

    同上

### 其他

我查看了一些微博评论, 其中有一些是回复其他人的, 因此评论正文中会有类似与@微博id的内容, 这些内容有固定的形式, 但它们所涉及到的词语很丰富, 这回干扰词向量的构建, 需要你使用正则表达式将他们除去, 你可以把这个过程抽象成一个方法, 然后将它写在util模块中, 还有一些我没有考虑到的问题, 如果你发现了请及时在群里一起讨论.

这里我只提供了一个大概的框架，具体的接口由你来设计